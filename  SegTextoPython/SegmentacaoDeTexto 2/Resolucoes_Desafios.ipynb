{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae996ddd",
   "metadata": {},
   "source": [
    "# âœ… ResoluÃ§Ã£o dos Desafios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e1becd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.pt.Portuguese at 0x2d67ed14830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c115b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Caminho do modelo Punkt local\n",
    "modelo_path = r\"D:\\SegmentacaoDePalavras\\punkt\\portuguese.pickle\"\n",
    "\n",
    "# Carregar o modelo Punkt\n",
    "with open(modelo_path, \"rb\") as f:\n",
    "    tokenizer_frases = pickle.load(f)\n",
    "\n",
    "# Tokenizador de palavras\n",
    "tokenizador_palavras = TreebankWordTokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8afc0d7",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Desafio 1 â€“ Frase mais longa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b037d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase mais longa: Esta aqui Ã© uma frase um pouco mais longa.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texto = \"Esta Ã© curta. Esta aqui Ã© uma frase um pouco mais longa. Curta tambÃ©m.\"\n",
    "\n",
    "frases = tokenizer_frases.tokenize(texto)\n",
    "frase_mais_longa = max(frases, key=lambda f: len(tokenizador_palavras.tokenize(f)))\n",
    "\n",
    "print(\"Frase mais longa:\", frase_mais_longa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77055e89",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Desafio 2 â€“ Filtrar frases curtas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c695a1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frases com 4 ou mais palavras:\n",
      "- Tudo bem contigo?\n",
      "- Vamos fazer algo produtivo.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texto = \"OlÃ¡. Tudo bem contigo? Vamos fazer algo produtivo. Sim!\"\n",
    "\n",
    "frases = tokenizer_frases.tokenize(texto)\n",
    "filtradas = [f for f in frases if len(tokenizador_palavras.tokenize(f)) >= 4]\n",
    "\n",
    "print(\"Frases com 4 ou mais palavras:\")\n",
    "for f in filtradas:\n",
    "    print(\"-\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e86d5",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Desafio 3 â€“ CSV com segmentaÃ§Ã£o por linha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24cbd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Frase\n",
      "0         Hoje estÃ¡ sol.\n",
      "1          AmanhÃ£ chove.\n",
      "2  Vamos estudar Python.\n",
      "3  Aprender Ã© divertido.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"texto\": [\"Hoje estÃ¡ sol. AmanhÃ£ chove.\", \"Vamos estudar Python. Aprender Ã© divertido.\"]\n",
    "})\n",
    "\n",
    "todas_frases = []\n",
    "for linha in df['texto']:\n",
    "    frases = tokenizer_frases.tokenize(linha)\n",
    "    todas_frases.extend(frases)\n",
    "\n",
    "df_frases = pd.DataFrame(todas_frases, columns=[\"Frase\"])\n",
    "df_frases.to_csv(\"frases_resultado.csv\", index=False, encoding=\"utf-8\")\n",
    "print(df_frases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a4fff",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Desafio 4 â€“ Comparar NLTK local vs spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7694373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - NLTK local: Vamos testar esta frase.\n",
      "    spaCy     : Vamos testar esta frase.\n",
      "\n",
      "2 - NLTK local: E esta outra tambÃ©m.\n",
      "    spaCy     : E esta outra tambÃ©m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "texto = \"Vamos testar esta frase. E esta outra tambÃ©m.\"\n",
    "\n",
    "frases_nltk = tokenizer_frases.tokenize(texto)\n",
    "doc = nlp(texto)\n",
    "frases_spacy = [sent.text for sent in doc.sents]\n",
    "\n",
    "for i, (nltk_f, spacy_f) in enumerate(zip(frases_nltk, frases_spacy), 1):\n",
    "    print(f\"{i} - NLTK local: {nltk_f}\")\n",
    "    print(f\"    spaCy     : {spacy_f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4c49e",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Desafio 5 â€“ Tkinter AvanÃ§ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafc2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tkinter as tk\n",
    "\n",
    "def analisar_texto():\n",
    "    texto = entrada.get(\"1.0\", \"end-1c\")\n",
    "    frases = tokenizer_frases.tokenize(texto)\n",
    "    total_frases = len(frases)\n",
    "    medias = [len(tokenizador_palavras.tokenize(f)) for f in frases]\n",
    "    media_palavras = sum(medias) / total_frases if total_frases > 0 else 0\n",
    "\n",
    "    saida.delete(\"1.0\", \"end\")\n",
    "    saida.insert(\"end\", f\"NÂº de frases: {total_frases}\\n\")\n",
    "    saida.insert(\"end\", f\"MÃ©dia de palavras por frase: {media_palavras:.2f}\\n\")\n",
    "\n",
    "janela = tk.Tk()\n",
    "janela.title(\"AnÃ¡lise AvanÃ§ada de Texto\")\n",
    "\n",
    "entrada = tk.Text(janela, height=6, width=60)\n",
    "entrada.pack(pady=10)\n",
    "\n",
    "botao = tk.Button(janela, text=\"Analisar Texto\", command=analisar_texto)\n",
    "botao.pack()\n",
    "\n",
    "saida = tk.Text(janela, height=6, width=60)\n",
    "saida.pack(pady=10)\n",
    "\n",
    "janela.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
